# ğŸ›°ï¸ Time Series Anomaly Detection for IoT Sensors

This project is an **end-to-end solution** for detecting anomalies in IoT sensor data using **Machine Learning and Deep Learning** techniques.  
It is developed as part of the **AI/ML Engineer (Fresher) Assignment**, demonstrating strong skills in data preprocessing, feature engineering, and anomaly detection modeling.

---

## ğŸ“˜ Overview

The project simulates a **run-to-failure scenario** using the **NASA Bearing Dataset** and builds two anomaly detection models:
- ğŸ§© **Isolation Forest (I-Forest)** â€” a tree-based unsupervised anomaly detector.
- ğŸ§  **LSTM Autoencoder** â€” a deep learning model for time-series reconstruction error detection.

The pipeline is fully modular and production-ready, making it easy to extend or integrate with real-world IoT systems.

---

## ğŸ§± Project Structure

```

your_project_folder/
â”œâ”€â”€ main.py                # The main script to run everything
â”œâ”€â”€ config.py              # Configuration and parameters
â”œâ”€â”€ data_loader.py         # Handles downloading, unzipping, and aggregation
â”œâ”€â”€ feature_engineering.py # Adds rolling stats and seasonal decomposition
â”œâ”€â”€ models.py              # Defines our I-Forest and Autoencoder models
â”œâ”€â”€ .gitignore             # Tells Git to ignore data and cache files
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ report.docx             # The page summary report
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ outputs/               # Auto-generated folder for all outputs
â”œâ”€â”€ plots/             # Saved plots and visualizations
â”œâ”€â”€ models/            # Trained models (I-Forest, LSTM Autoencoder)
â””â”€â”€ processed_data/    # Intermediate aggregated/cleaned data

````

---

## âš™ï¸ How to Run

### 1ï¸âƒ£ Install Dependencies
First, install the required Python libraries:
```bash
pip install -r requirements.txt
````

---

### 2ï¸âƒ£ Download the Data

You have two options:

#### ğŸ”¹ Automatic Download (using Kaggle API)

If you have your Kaggle API token (`kaggle.json`) in your `~/.kaggle/` directory,
the script will automatically download the dataset:

```
vinayak123tyagi/bearing-dataset
```

#### ğŸ”¹ Manual Download

Alternatively, manually download **bearing-dataset.zip** from Kaggle and place it in the project root folder.
The script will automatically detect it and skip the download step.

---

### 3ï¸âƒ£ Run the Full Pipeline

Execute the main script:

```bash
python main.py
```

The pipeline performs the following steps with detailed console logs:

1. Sets up logging at `outputs/pipeline.log`
2. Checks and processes raw data into a clean time series
3. Generates EDA plots (distributions, run-to-failure) â†’ `outputs/plots/`
4. Performs feature engineering (rolling stats, seasonal decomposition)
5. Trains and evaluates:

   * Isolation Forest
   * LSTM Autoencoder
6. Saves trained models â†’ `outputs/models/`
7. Generates final anomaly comparison plots â†’ `outputs/plots/`

---

## ğŸ§© Key Files

| File                       | Description                                              |
| -------------------------- | -------------------------------------------------------- |
| `main.py`                  | Central orchestrator for the entire pipeline             |
| `config.py`                | Stores parameters, paths, and constants                  |
| `data_loader.py`           | Handles dataset loading, unzipping, and preprocessing    |
| `feature_engineering.py`   | Adds rolling statistics and decomposed features          |
| `models.py`                | Defines the Isolation Forest and LSTM Autoencoder models |
| `report.md` / `report.pdf` | 2â€“3 page final summary report with analysis              |

---

## ğŸ“Š Outputs

All generated files are automatically organized in the `outputs/` directory:

```
outputs/
â”œâ”€â”€ plots/           # Visualization of trends and anomalies
â”œâ”€â”€ models/          # Saved model weights and configurations
â””â”€â”€ processed_data/  # Intermediate cleaned and aggregated datasets
```

---

## ğŸ§  Tech Stack

* **Python 3.x**
* **NumPy**, **Pandas**, **Matplotlib**, **Seaborn**
* **Scikit-learn**
* **TensorFlow / Keras**
* **Statsmodels**
* **Kaggle API**

---

## ğŸ§¾ Report

The detailed project summary, including methodology, models, results, and visual insights,
is available in:
* `report.docx`

---

## ğŸ“¬ Author

ğŸ‘¤ **Pushpanathan N**
ğŸ“§ [GitHub Profile](https://github.com/Masterpush07)

---

## â­ Acknowledgements

* NASA Bearing Dataset â€” *for providing run-to-failure sensor data*
* Kaggle â€” *for dataset hosting and API integration support*

---

> â€œAnomalies arenâ€™t just outliers â€” theyâ€™re stories waiting to be understood.â€ ğŸ’¡

